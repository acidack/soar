{
    "lastRunStatus": 0,
    "lastRunTime": 1716291075777,
    "uniqueIdentifier": "29c5203c-269d-4d7b-9611-4d0322033826",
    "id": 0,
    "jobDefinitionId": 0,
    "name": "Google Chronicle Sync Job",
    "integration": "GoogleChronicle",
    "script": "from consts import (\n    DEFAULT_HOURS_BACKWARDS,\n    INTEGRATION_DISPLAY_NAME,\n    MAX_FETCH_LIMIT_FOR_JOB,\n    SYNC_DATA_SCRIPT_NAME,\n    UNIFIED_CONNECTOR_DEVICE_VENDOR,\n)\nfrom exceptions import (\n    GoogleChronicleValidationError,\n    GoogleChroniclePlatformUnsupportedError\n)\nfrom GoogleChronicleManager import GoogleChronicleManager\nfrom SiemplifyDataModel import SyncCaseIdMatch\nfrom SiemplifyJob import SiemplifyJob\nfrom SiemplifyUtils import output_handler, unix_now\nfrom TIPCommon import extract_action_param\nfrom utils import (\n    UNIX_FORMAT,\n    get_last_success_time_for_job,\n    platform_supports_chronicle,\n    platform_supports_uno_3,\n    read_ids_for_job,\n    save_timestamp_for_job,\n    write_ids_for_job,\n)\n\n\nCASE_IDS_DB_KEY = \"pending_case_ids\"\nALERT_IDS_DB_KEY = \"pending_alert_ids\"\nCASES_TIMESTAMP_DB_KEY = \"cases_timestamp\"\nALERTS_TIMESTAMP_DB_KEY = \"alerts_timestamp\"\nMAX_RETRIES_NUMBER = 5\nMINIMUM_SUPPORTED_VERSION = \"6.1.44\"\n\n\ndef get_vendor_filter(siemplify):\n    \"\"\"Returns the appropriate vendor to use as filter when querying the SDK\n\n    for updated cases/alerts.\n\n    If the SDK supports latest UNO Phase 3 changes, no filtering is needed.\n    Otherwise, use the defualt unified chronicle vendor\n\n    Args:\n        siemplify (Siemplify): The SDK object\n\n    Returns:\n        str | None: vendor to use as filter for fetching updated cases/alerts.\n    \"\"\"\n    return (\n        None\n        if platform_supports_uno_3(siemplify)\n        else UNIFIED_CONNECTOR_DEVICE_VENDOR\n    )\n\n\n@output_handler\ndef main():\n    siemplify = SiemplifyJob()\n    siemplify.script_name = SYNC_DATA_SCRIPT_NAME\n    siemplify.LOGGER.info(\"--------------- JOB STARTED ---------------\")\n\n    environment = extract_action_param(\n        siemplify=siemplify,\n        param_name=\"Environment\",\n        is_mandatory=True,\n        print_value=True,\n        default_value=\"Default Environment\",\n    )\n    api_root = extract_action_param(\n        siemplify=siemplify,\n        param_name=\"API Root\",\n        is_mandatory=True,\n        print_value=True,\n    )\n    creds = extract_action_param(\n        siemplify=siemplify,\n        param_name=\"User's Service Account\",\n        print_value=False,\n        remove_whitespaces=False\n    )\n    hours_backwards = extract_action_param(\n        siemplify=siemplify,\n        param_name=\"Max Hours Backwards\",\n        input_type=int,\n        print_value=True,\n        default_value=DEFAULT_HOURS_BACKWARDS,\n    )\n    verify_ssl = extract_action_param(\n        siemplify=siemplify,\n        param_name=\"Verify SSL\",\n        default_value=True,\n        input_type=bool,\n        print_value=True,\n    )\n\n    siemplify.LOGGER.info(\"----------------- Main - Started -----------------\")\n\n    try:\n        if hours_backwards < 1:\n            raise GoogleChronicleValidationError(\n                '\"Max Hours Backwards\" parameter must be a positive number.'\n            )\n\n        if not platform_supports_chronicle(siemplify):\n            raise GoogleChroniclePlatformUnsupportedError(\n                \"Current platform version does not support SDK methods\"\n                f\"designed for {INTEGRATION_DISPLAY_NAME}. \"\n                f\"Please use version {MINIMUM_SUPPORTED_VERSION} or higher.\"\n            )\n\n        manager = GoogleChronicleManager.create_manager_instance(\n            user_service_account=creds,\n            chronicle_soar=siemplify,\n            api_root=api_root,\n            verify_ssl=verify_ssl,\n        )\n        manager.test_connectivity()\n\n        siemplify.LOGGER.info(\"--- Start Processing Updated Cases ---\")\n\n        cases_last_success_timestamp = get_last_success_time_for_job(\n            siemplify=siemplify,\n            offset_with_metric={\"hours\": hours_backwards},\n            time_format=UNIX_FORMAT,\n            timestamp_key=CASES_TIMESTAMP_DB_KEY,\n        )\n\n        pending_case_ids = read_ids_for_job(\n            siemplify, CASE_IDS_DB_KEY, default_value_to_return=[]\n        )\n        siemplify.LOGGER.info(\n            f\"Successfully loaded {len(pending_case_ids)} pending case ids\"\n        )\n\n        cases_count = MAX_FETCH_LIMIT_FOR_JOB - len(pending_case_ids)\n        if cases_count <= 0:\n            if cases_count < 0:\n                siemplify.LOGGER.error(\n                    f\"Cases overload: case limit is {MAX_FETCH_LIMIT_FOR_JOB}. \"\n                    f\"{abs(cases_count)} cases will not be synced\"\n                )\n                pending_case_ids = pending_case_ids[:MAX_FETCH_LIMIT_FOR_JOB]\n            cases_metadata = []\n        else:\n            cases_metadata = manager.get_updated_cases_metadata(\n                siemplify=siemplify,\n                start_timestamp_unix_ms=cases_last_success_timestamp,\n                count=cases_count,\n                allowed_environments=[environment],\n                vendor=get_vendor_filter(siemplify),\n            )\n            siemplify.LOGGER.info(\n                f\"Found {cases_metadata} updated cases since last fetch time.\"\n            )\n\n        case_ids = [\n            pending_case_id.get(\"id\") for pending_case_id in pending_case_ids\n        ]\n        case_ids += [metadata.id for metadata in cases_metadata]\n        case_ids = list(set(case_ids))\n        cases_with_details = siemplify.get_sync_cases(case_ids)\n        chronicle_cases = manager.convert_siemplify_cases_to_chronicle(\n            cases_with_details\n        )\n        successful_cases = []\n        failed_cases = []\n        cases_to_remove_from_backlog = []\n\n        if case_ids:\n            siemplify.LOGGER.info(\"--- Start Updating Cases in Chronicle ---\")\n            siemplify.LOGGER.info(f\"Case ids to be updated: {case_ids}\")\n            try:\n                updated_chronicle_cases = (\n                    manager.batch_update_cases_in_chronicle(chronicle_cases)\n                )\n\n                for case in updated_chronicle_cases:\n                    if case.has_failed:\n                        failed_cases.append(case)\n                    else:\n                        successful_cases.append(case)\n\n                for failed_case in failed_cases:\n                    pending_case = next(\n                        (\n                            item\n                            for item in pending_case_ids\n                            if item.get(\"id\") == failed_case.id\n                        ),\n                        {},\n                    )\n                    retries_counter = pending_case.get(\"retries\", 0)\n                    if retries_counter >= MAX_RETRIES_NUMBER:\n                        siemplify.LOGGER.info(\n                            f\"Max retries reached for case {failed_case.id}.\"\n                            \" Removing from backlog.\"\n                        )\n                        cases_to_remove_from_backlog.append(failed_case.id)\n                        continue\n                    else:\n                        if pending_case:\n                            pending_case[\"retries\"] += 1\n                        else:\n                            pending_case_ids.append(\n                                {\"id\": failed_case.id, \"retries\": 1}\n                            )\n\n                if failed_cases:\n                    siemplify.LOGGER.info(\n                        \"The following cases were not synced:\"\n                        f\" {', '.join([str(failed.id) for failed in failed_cases])}. \"\n                    )\n\n                case_id_matches = []\n                for successful_case in successful_cases:\n                    ids = [case.get(\"id\") for case in pending_case_ids]\n                    if successful_case.id in ids:\n                        cases_to_remove_from_backlog.append(successful_case.id)\n\n                    for chr_case in chronicle_cases:\n                        if (\n                            successful_case.id == chr_case.id\n                            and chr_case.external_id in [\"None\", None, \"\"]\n                        ):\n                            case_id_matches.append(\n                                SyncCaseIdMatch(\n                                    successful_case.id,\n                                    successful_case.external_id,\n                                )\n                            )\n                try:\n                    updated_case_ids = siemplify.batch_update_case_id_matches(\n                        case_id_matches\n                    )\n                    siemplify.LOGGER.info(\n                        \"Updated External Case Ids for the following cases:\"\n                        f\" {updated_case_ids}\"\n                    )\n                except Exception as e:\n                    siemplify.LOGGER.error(\"Failed to update external ids.\")\n                    siemplify.LOGGER.exception(e)\n            except Exception as e:\n                siemplify.LOGGER.error(\"Failed to update cases in Chronicle.\")\n                siemplify.LOGGER.exception(e)\n            siemplify.LOGGER.info(\n                \"--- Finished Updating Cases in Chronicle ---\"\n            )\n\n        pending_case_ids = [\n            pending\n            for pending in pending_case_ids\n            if pending.get(\"id\") not in cases_to_remove_from_backlog\n        ]\n        if pending_case_ids not in [{}, [], None]:\n            siemplify.LOGGER.info(\n                \"The following failed case ids were put in the backlog: \"\n                f\"{', '.join([str(pending.get('id')) for pending in pending_case_ids])}\"\n            )\n        siemplify.LOGGER.info(\"--- Finished Processing Updated Cases ---\")\n\n        siemplify.LOGGER.info(\"--- Start Processing Updated Alerts ---\")\n\n        alerts_last_success_timestamp = get_last_success_time_for_job(\n            siemplify=siemplify,\n            offset_with_metric={\"hours\": hours_backwards},\n            time_format=UNIX_FORMAT,\n            timestamp_key=ALERTS_TIMESTAMP_DB_KEY,\n        )\n\n        pending_alert_ids = read_ids_for_job(\n            siemplify, ALERT_IDS_DB_KEY, default_value_to_return=[]\n        )\n        siemplify.LOGGER.info(\n            f\"Successfully loaded {len(pending_alert_ids)} pending alert ids\"\n        )\n\n        alerts_count = MAX_FETCH_LIMIT_FOR_JOB - len(pending_alert_ids)\n        if alerts_count <= 0:\n            if alerts_count < 0:\n                siemplify.LOGGER.error(\n                    \"Alerts overload: alert limit is\"\n                    f\" {MAX_FETCH_LIMIT_FOR_JOB}. {abs(alerts_count)} alerts\"\n                    \" will not be synced\"\n                )\n                pending_alert_ids = pending_alert_ids[:MAX_FETCH_LIMIT_FOR_JOB]\n            alerts_metadata = []\n        else:\n            alerts_metadata = manager.get_updated_alerts_metadata(\n                siemplify=siemplify,\n                start_timestamp_unix_ms=alerts_last_success_timestamp,\n                count=alerts_count,\n                allowed_environments=[environment],\n                vendor=get_vendor_filter(siemplify),\n                include_non_synced_alerts=(\n                    False if platform_supports_uno_3(siemplify) else True\n                ),\n            )\n            siemplify.LOGGER.info(\n                f\"Found {len(alerts_metadata)} updated alerts \"\n                \"since last fetch time.\"\n            )\n\n        alert_ids = [\n            pending_alert_id.get(\"group_id\")\n            for pending_alert_id in pending_alert_ids\n        ]\n        alert_ids += [metadata.group_id for metadata in alerts_metadata]\n        alert_ids = list(set(alert_ids))\n        alerts_with_details = siemplify.get_sync_alerts(alert_ids)\n\n        alerts_cases_with_external_ids = siemplify.get_sync_cases(\n            [sync_alert.case_id for sync_alert in alerts_with_details]\n        )\n\n        chronicle_alerts = manager.convert_siemplify_alerts_to_chronicle(\n            alerts_with_details, alerts_cases_with_external_ids\n        )\n\n        successful_alerts = []\n        failed_alerts = []\n        alerts_to_remove_from_backlog = []\n\n        if alert_ids:\n            siemplify.LOGGER.info(\"--- Start Updating Alerts in Chronicle ---\")\n            siemplify.LOGGER.info(f\"Alert ids to be updated: {alert_ids}\")\n            try:\n                updated_chronicle_alerts = (\n                    manager.batch_update_alerts_in_chronicle(chronicle_alerts)\n                )\n\n                for alert in updated_chronicle_alerts:\n                    if alert.has_failed:\n                        failed_alerts.append(alert)\n                    else:\n                        successful_alerts.append(alert)\n\n                for failed_alert in failed_alerts:\n                    pending_alert = next(\n                        (\n                            item\n                            for item in pending_alert_ids\n                            if item.get(\"group_id\") == failed_alert.group_id\n                        ),\n                        {},\n                    )\n                    retries_counter = pending_alert.get(\"retries\", 0)\n                    if retries_counter >= MAX_RETRIES_NUMBER:\n                        siemplify.LOGGER.info(\n                            \"Max retries reached for alert\"\n                            f\" {failed_alert.group_id}. Removing from backlog.\"\n                        )\n                        alerts_to_remove_from_backlog.append(\n                            failed_alert.group_id\n                        )\n                        continue\n                    else:\n                        if pending_alert:\n                            pending_alert[\"retries\"] += 1\n                        else:\n                            pending_alert_ids.append({\n                                \"group_id\": failed_alert.group_id,\n                                \"retries\": 1,\n                            })\n\n                if failed_alerts:\n                    siemplify.LOGGER.info(\n                        \"The following alerts were not synced:\"\n                        f\" {', '.join([failed.group_id for failed in failed_alerts])}.\"\n                        \" IDs are put into the backlog.\"\n                    )\n\n                for successful_alert in successful_alerts:\n                    ids = [alert.get(\"group_id\") for alert in pending_alert_ids]\n                    if successful_alert.group_id in ids:\n                        alerts_to_remove_from_backlog.append(\n                            successful_alert.group_id\n                        )\n\n            except Exception as e:\n                siemplify.LOGGER.error(\"Failed to update alerts in Chronicle.\")\n                siemplify.LOGGER.exception(e)\n            siemplify.LOGGER.info(\n                \"--- Finished Updating Alerts in Chronicle ---\"\n            )\n\n        pending_alert_ids = [\n            pending\n            for pending in pending_alert_ids\n            if pending.get(\"group_id\") not in alerts_to_remove_from_backlog\n        ]\n        siemplify.LOGGER.info(\"--- Finished Processing Updated Alerts ---\")\n\n        all_cases = sorted(cases_metadata, key=lambda item: item.tracking_time)\n        all_alerts = sorted(\n            alerts_metadata, key=lambda item: item.tracking_time\n        )\n\n        siemplify.LOGGER.info(\"Saving timestamps.\")\n        if successful_cases + failed_cases and all_cases:\n            new_timestamp_for_cases = all_cases[-1].tracking_time\n            save_timestamp_for_job(\n                siemplify,\n                new_timestamp=new_timestamp_for_cases,\n                timestamp_key=CASES_TIMESTAMP_DB_KEY,\n            )\n        if successful_alerts + failed_alerts and all_alerts:\n            new_timestamp_for_alerts = all_alerts[-1].tracking_time\n            save_timestamp_for_job(\n                siemplify,\n                new_timestamp=new_timestamp_for_alerts,\n                timestamp_key=ALERTS_TIMESTAMP_DB_KEY,\n            )\n\n        siemplify.LOGGER.info(\"Saving pending ids.\")\n        write_ids_for_job(\n            siemplify, content_to_write=pending_case_ids, db_key=CASE_IDS_DB_KEY\n        )\n        write_ids_for_job(\n            siemplify,\n            content_to_write=pending_alert_ids,\n            db_key=ALERT_IDS_DB_KEY,\n        )\n\n        siemplify.LOGGER.info(\"--------------- JOB FINISHED ---------------\")\n\n    except Exception as error:\n        siemplify.LOGGER.error(f\"Got exception on main handler. Error: {error}\")\n        siemplify.LOGGER.exception(error)\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "creator": "Admin",
    "description": "This job will synchronize information about Chronicle SOAR Cases and Chronicle SOAR Alerts with Chronicle SIEM.\n Note: This job is only supported from Chronicle SOAR version 6.1.44 and higher.",
    "isEnabled": true,
    "isCustom": false,
    "version": 1,
    "parameters": [
        {
            "id": 10,
            "isMandatory": true,
            "name": "Environment",
            "type": 2,
            "value": "Default Environment"
        },
        {
            "id": 11,
            "isMandatory": true,
            "name": "API Root",
            "type": 2,
            "value": "https://backstory.googleapis.com"
        },
        {
            "id": 12,
            "isMandatory": false,
            "name": "User's Service Account",
            "type": 3,
            "value": "***************"
        },
        {
            "id": 13,
            "isMandatory": false,
            "name": "Max Hours Backwards",
            "type": 2,
            "value": "24"
        },
        {
            "id": 14,
            "isMandatory": false,
            "name": "Verify SSL",
            "type": 0,
            "value": "true"
        }
    ],
    "runIntervalInSeconds": 300,
    "creationTime": "2024-03-13T11:08:58.255Z",
    "lastModificationTime": "2024-04-21T07:26:49.818Z",
    "isSystemJob": false,
    "jobDefinitionName": "Google Chronicle Sync Job",
    "agentIdentifier": null
}